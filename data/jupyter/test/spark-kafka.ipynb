{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f71516-a94e-4422-8715-000301e771b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.3,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 pyspark-shell'\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import  SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 pyspark-shell'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea2076-c65b-4770-a3ce-4b19441a1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_conf = SparkConf() \\\n",
    "            .setMaster(\"spark://spark-master:7077\") \\\n",
    "            .setAppName(\"MyApp3\") \\\n",
    "            .setAll([('spark.executor.memory', '4g'),\n",
    "                     ('spark.executor.cores', '4'), \n",
    "                     ('spark.cores.max', '4'), \n",
    "                     ('spark.driver.memory','4g'),\n",
    "                     ('spark.driver.host','192.168.96.3')\n",
    "            ])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(conf=spark_conf) \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d7d5a-263b-4f29-a4bb-34775e3d38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c5c81-c418-48c9-a487-60a3c8a4d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"mytopic\"\n",
    "broker = \"kafka:9092\"\n",
    "\n",
    "directKafkaStream = KafkaUtils.createDirectStream(ssc, topics=[topic], kafkaParams={\"metadata.broker.list\": broker})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35024db7-f2c7-4b18-a787-ab36cb8e1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "offsetRanges = []\n",
    "\n",
    "def storeOffsetRanges(rdd):\n",
    "    global offsetRanges\n",
    "    offsetRanges = rdd.offsetRanges()\n",
    "    return rdd\n",
    "\n",
    "def printOffsetRanges(rdd):\n",
    "    for o in offsetRanges:\n",
    "        print(\"o: \", o)\n",
    "        pprint(vars(o))\n",
    "        print(o.topic, o.value, o.partition, o.fromOffset, o.untilOffset)\n",
    "\n",
    "\n",
    "directKafkaStream.transform(storeOffsetRanges).foreachRDD(printOffsetRanges)\n",
    "ssc.start() \n",
    "ssc.awaitTermination() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502110c-7ad0-432d-9751-471fa6a5e626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94aa9df-918d-472a-bccc-80ba7685be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = directKafkaStream \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b266d-d657-4fa2-9cc1-bc79cf747b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c49d5-b8ab-46af-8e01-bcad4e9bfeda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cee25e-66a4-4d4c-b471-1da0bcc8fc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479468eb-5de7-4f6b-b4ad-f5fe4dc35b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc,2)\n",
    "topic = \"mytopic\"\n",
    "# kvs = KafkaUtils.createStream(ssc,\"kafka:9092\",\"raw-event-streaming-consumer\",{topic:1})\n",
    "dks = KafkaUtils.createDirectStream(ssc, [topic], {\"metadata.broker.list\": brokers})\n",
    "\n",
    "# lines = kvs.map(lambda x: x[1])\n",
    "lines = dks.map(lambda x: x[1])\n",
    "\n",
    "lines.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c58e5b6-588f-4ee5-92ed-4c9dbb621106",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = directKafkaStream.map(lambda x: x[1]).flatMap(lambda x: x.split(\" \"))\n",
    "wordcount = words.map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "wordcount.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51851a-f74b-417f-9faf-916f66a13e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af824cf5-6329-4100-af66-5022702f39e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "018ec8de-474a-4d00-ae23-2d8b1fc0fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#original input schema\n",
    "jsonSchema = (\n",
    "  StructType()\n",
    "  .add(\"timestamp\", TimestampType()) #event time at the source\n",
    "  .add(\"deviceId\", LongType())\n",
    "  .add(\"deviceType\", StringType())\n",
    "  .add(\"signalStrength\", DoubleType())\n",
    ")\n",
    "# modified schema with added columns since we are \n",
    "# doing some ETL (transforming and adding extra columns)\n",
    "# this transformed data will be stored into parquet files\n",
    "# from which an SQL table can be created for consumption or\n",
    "# report generation\n",
    "parquetSchema = (\n",
    "  StructType()\n",
    "  .add(\"timestamp\", TimestampType()) #event time at the source\n",
    "  .add(\"deviceId\", LongType())\n",
    "  .add(\"deviceType\", StringType())\n",
    "  .add(\"signalStrength\", DoubleType())\n",
    "  .add(\"INPUT_FILE_NAME\", StringType()) #file name from which this data item was read\n",
    "  .add(\"PROCESSED_TIME\", TimestampType())) #time at the executor while processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5240b93-64a5-40a5-ac69-7ebec4997321",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = ( spark \n",
    "          .readStream \n",
    "          .format(\"kafka\")\n",
    "          .schema(jsonSchema) \n",
    "          .option(\"maxFilesPerTrigger\", 1)  #slow it down for tutorial\n",
    "          .option(\"badRecordsPath\", bad_records_path) #any bad records will go here\n",
    "          .json(sensor_path) #the source\n",
    "          .withColumn(\"INPUT_FILE_NAME\", input_file_name()) #maintain file path\n",
    "          .withColumn(\"PROCESSED_TIME\", current_timestamp()) #add a processing timestamp at the time of processing\n",
    "          .withWatermark(\"PROCESSED_TIME\", \"1 minute\") #optional: window for out of order data\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d19e5a-0992-48ee-9cf7-24ccb32b79e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0de88e-a3d9-40cb-9d8e-045d3d4d2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyspark --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.3,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd4fdc88-fdb6-4545-bea7-74388c0ec7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "  .option(\"subscribe\", \"topic1\")\n",
    "  .load()\n",
    "#   .select(from_json(col(\"value\").cast(\"string\"), schema, jsonOptions).alias(\"parsed_value\"))\n",
    ")\n",
    "\n",
    "parsed.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8566eed-f462-4a1b-8ce6-c0afef34d2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9ba19ab-85b1-4600-88c4-84767580a516",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Queries with streaming sources must be executed with writeStream.start();;\\nkafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o543.collectToPython.\n: org.apache.spark.sql.AnalysisException: Queries with streaming sources must be executed with writeStream.start();;\nkafka\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:389)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:36)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:36)\n\tat org.apache.spark.sql.execution.QueryExecution.assertSupported(QueryExecution.scala:51)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:62)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:60)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3359)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14311/3934993760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Queries with streaming sources must be executed with writeStream.start();;\\nkafka'"
     ]
    }
   ],
   "source": [
    "parsed.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92740f-161f-497a-8d75-a644f262571e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d648f0-e316-42c4-a826-3497a8ce64d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defafe0-189a-4f43-aed9-4b95c3eb9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df2e60-29be-400c-bdc1-e19bef46e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()\n",
    "    \n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f02d1-be06-4253-aa37-333957cefdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./output_path/\"\n",
    "checkpoint_path = \"./checkpoint_path/\"\n",
    "\n",
    "query = (inputDF\n",
    "         .writeStream\n",
    "         .format(\"parquet\") #our sink to save it for posterity or batch queries if needed\n",
    "         .option(\"path\", output_path)\n",
    "         .option(\"checkpointLocation\", checkpoint_path) # add checkpointing for resiliency\n",
    "         .outputMode(\"append\")\n",
    "         .start() \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a05fe6-410e-4137-b327-8cc65b894872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7d5cd-742b-413b-b153-0a5101c0346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76757406-ba59-44c8-aa18-2bd02473a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466cd92-6701-406a-bb17-0592e75a0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02799dd9-a174-42ff-b218-cd67eeee0150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edef64-2fe0-4a2c-bc07-a96e677dbce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71efc9-1c18-4b19-a731-81220b93522c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca618f-070b-4133-8f1b-4478665203f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30a1a0fc-de91-4456-b9ce-9f8319e9bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .config(conf=spark_conf) \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "919a3379-a4a6-4487-8eb6-52b1b3766575",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_det_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"subscribe\", \"topic1\") \\\n",
    "        .option(\"startingOffsets\", \"latest\") \\\n",
    "        .load() \\\n",
    "        .selectExpr(\"CAST(value as STRING)\", \"CAST(timestamp as STRING)\",\"CAST(topic as STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03256bf2-24d6-40c4-aad5-4ecf98a6dbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+---------+-----+\n",
      "|value|timestamp|topic|\n",
      "+-----+---------+-----+\n",
      "+-----+---------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------+-----------------------+------+\n",
      "|value       |timestamp              |topic |\n",
      "+------------+-----------------------+------+\n",
      "|next message|2021-08-05 16:06:29.073|topic1|\n",
      "+------------+-----------------------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------+-----------------------+------+\n",
      "|value       |timestamp              |topic |\n",
      "+------------+-----------------------+------+\n",
      "|next message|2021-08-05 16:06:46.776|topic1|\n",
      "+------------+-----------------------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------+-----------------------+------+\n",
      "|value       |timestamp              |topic |\n",
      "+------------+-----------------------+------+\n",
      "|next message|2021-08-05 16:07:46.067|topic1|\n",
      "+------------+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = trans_det_df.writeStream \\\n",
    "            .format(\"console\") \\\n",
    "            .option(\"truncate\",\"false\") \\\n",
    "            .start() \\\n",
    "            .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bc990-8c2f-4bed-8849-d6fd00b47a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
